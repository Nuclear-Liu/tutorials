# LMAX Disruptor：用于在并发线程之间交换数据的有界队列的高性能替代方案 (High performance alternative to bounded queues for exchanging data between concurrent threads)

> Martin Thompson · Dave Farley · Michael Barker · Patricia Gee · Andrew Stewart - Version 4.0.0.RC2-SNAPSHOT, May 2011

## 摘要 Abstract

LMAX 的成立是为了创造一个非常高性能的金融交易所。
作为实现这一目标的工作的一部分，我们评估了设计此类系统的几种方法，但是当我们开始测量这些方法时，我们遇到了传统方法的一些基本限制。

许多应用进程依赖队列在处理阶段之间交换数据。
我们的性能测试表明，以这种方式使用队列时，延迟成本对磁盘（基于 RAID 或 SSD 的磁盘系统）的 IO 操作成本处于同一数量级 —— 非常慢。
如果端到端操作中有多个队列，这将增加数百微妙的整体延迟。
显然还有优化的空间。

进一步调查对计算机科学的关注使我们意识到传统方法（例如队列和处理节点）中固有的关注点的混论导致多线程实现中的争用，这表明可能有更好的方法。

考虑现在 CPU 的工作方式，我们喜欢称之为”机械同情“("mechanical sympathy")，使用良好的设计实践，重点关注梳理问题，我们提出了一种数据结构和一种我们称之为 Disruptor 的使用模式。

测试表明，将 Disruptor 用于三级管道的平均延迟比等效的基于队列的方法低 3 个数量级。
此外，对于相同的配置， Disruptor 处理的吞吐量大约高出 8 倍。

这些性能改进代表了围绕并发编程的思想发生了重大变化。
这种新模式是任何需要高吞吐量和低延迟的异步事件处理框架的理想基础。

在 LMAX ，我们创建了一个订单匹配引擎、实施风险管理和一个高可用的内存交易处理系统，所有这些都是在这个模式上取得了巨大的成功。
这些系统中的每一个都设定了新的性能指标，据我们所知，这些标准是无与伦比的。

然而，这不仅与金融行业相关的专业解决方案。
Disruptor 是一种通用机制，它以最大化性能的方式解决并发编程中的复杂问题，并且易于实现。
尽管一些概念可能看起来不寻常，但根据我们的经验，构建这种模式的系统比类似的机制更容易实现。

与同类方法相比， Disruptor 具有更少的写入争用、更低的并发开销并且对缓存更友好，所有这些都导致更高的吞吐量以及更低的延迟和更少的抖动。
在中等时钟速率的处理器上，我们已经看到每秒超过 2500 万消息和低于 50 纳秒的延迟。
与我们见过的任何其他实现相比，这种性能是一个显著的改进。
这非常接近处理器在内核之间交换数据的理论极限。

## 1. 概述 Overview

Disruptor 是我们努力在 LMAX 创建世界上最高性能的金融交易所的结果。
早期的设计侧重于 `SEDA` 和 `Actor` 派生的架构，使用管道来提高吞吐量。
在分析各种实现之后，很明显，管道中各个阶段之间的事件队列主导了成本。
我们发现队列还引入了延迟和高抖动。
我们花费了大量精力来开发具有更好性能的新队列实现。
然而，很明显，由于生产者、消费者及其数据存储设计问题混为一谈，队列作为一种基本数据结构是有限的。
Disruptor 是我们努力创建一个并发结构的结果，该结构可以干净地分离这些问题。

## 2. 并发的复杂性 The Complexities of Concurrency

在本文档和一般计算机科学的上下文中，并发不仅意味着两个或多个任务并行发生，而且还意味他们争夺资源访问权。
争用的资源可能是数据库、文档、套接字甚至是内存中的某个位置。

代码的并发执行与两件事有关，**互斥**和**变化的可见性**。
**互斥**是关于管理对某些资源的竞争更新。
**更改的可见性**是关于控制何时使这些更改对其他线程可见。
如果您可以消除竞争更新的需要，则可以避免互斥的需要。
如果你的算法可以保证任何给定资源只被一个线程修改，那么互斥就没有必要了。
读写操作要求所有更改对其他线程可见。
然而，只有争用的写操作需要更改的互斥。

在任何并发环境中，成本最高的操作是竞争写访问。
让多个线程写入同一资源需要复杂且昂贵的协调。
通常这是通过采用某种锁定策略来实现的。

## 2.1. 锁的成本 The Cost of Locks

**锁**提供互斥并确保更改的可见性以及有序的方式发生。
锁是非常昂贵的，因为它们在争用时需要仲裁。
这种仲裁是通过上下文切换到操作系统内核来实现的，操作系统内核将挂起等待锁的线程，直到它被释放。
在这样的上下文切换期间，以及将控制权释放给操作系统，操作系统可能决定在它拥有控制权的同时执行其他内务处理任务。执行上下文可能会丢失先前缓存的数据和指令。
这对现代处理器产生严重的性能影响。
可以使用**快速用户模式锁**，但这些只有在不争用时才有真正的好处。

我们将通过一个简单的演示来说明锁的成本。
本实验的重点是调用了一个函数，该函数在循环中将 64 位计数器递增 5 亿次。
如果用 Java 编写，这可以由 2.4Ghz Intel Westmere EP 上的单个线程执行，只需 300 毫秒。
语言对于这个实验并不重要，所有具有相同基本原语的结果都是相似的。

一旦引入锁来提供互斥，即使锁尚未竞争，成本也会显著上升。
当两个或更多线程开始竞争时，成本会再次增加，数量级会增加。
这个简单实验的结果如下表所示：

_Table 1. 争用的比较成本 Comparative costs of contention_

| Method                            | Time(ms) |
|-----------------------------------|---------:|
| Single thread                     |      300 |
| Single thread with lock           |   10,000 |
| Two threads with lock             |  224,000 |
| Single thread with CAS            |    5,700 |
| Two threads with CAS              |   30,000 |
| Single thread with volatile write |    4,700 |

## 2.2. "CAS" 的成本 The Costs of "CAS"

当更新的目标时单个字时，可以采用一种比使用锁更有效的替代方法来更新内存。
这些替代方案基于在现代处理器中实现的原子或互锁指令。
这些通常称为 CAS（比较和交换）操作，例如 x86 上的 `lock cmpxchg` 。
CAS 操作是一种特殊的机器代码指令，它允许将内存中的一个字有条件地设置为原子操作。
对于**"增加计数器实验"**，每个线程都可以在循环中旋转读取计数器，然后尝试以原子方式将其设置为新的增量值。
旧值和新值作为参数提供给该指令。
如果在执行操作时，计数器的值与提供的预期值匹配，则用新值更新计数器。
另一方面，如果该值不符合预期，则 CAS 操作将失败。
然后由尝试执行更改的线程重试，重新读取从该值递增的计数器，以此类推，直到更改成功。
这种 CAS 方法比锁更有效，因此它不需要上下文切换到内核进行仲裁。
然而， CAS 操作不是免费的。
处理器必须锁定其指令管道以确保原子性，并使用内存屏障使更改对其它线程可见。
通过使用 `java.util.concurrent.Atomic*` 类， CAS 操作在 Java 中可用。

如果进程的关键部分比简单的计数器增量更复杂，则可能需要使用多个 CAS 操作的复杂状态机来协调争用。
使用锁开发并发进程是困难的；使用 CAS 操作和内存屏障开发无锁算法要复杂很多倍，而且很难证明它们是正确的。

理想的算法是只有一个线程拥有对单个资源的所有写入，而其他线程读取结果。
要在多处理器环境中读取结果，需要内存屏障以使更改对在其他处理器上运行的线程可见。

## 2.3. 内存屏障 Memory Barriers

处于性能原因，现代处理器执行指令的乱序执行以及内存和执行单元之间数据的乱序加载和存储。
处理器只需要保证进程逻辑产生相同的结果，而不管执行顺序如何。
这不是单线程进程的问题。
然而，当线程共享状态时，重要的是所有内存更改在所需要的时间点按顺序出现，以便数据交换成功。
处理器使用内存屏障来指示内存更新顺序很重要的代码部分。
它们是在线程之间实现硬件排序和更改可见性的方法。
编译器可以设置补充的软件屏障来确保编译代码的顺序，这样的软件内存屏障是处理器本身使用的硬件屏障的补充。

现代的 CPU 现在比当前一代的内存系统快得多。
为了弥合这种鸿沟， CPU 使用复杂的缓存熊，这些系统实际上是没有链接的快速硬件哈希表。
这些缓存通过消息传递协议与其他处理器缓存系统保持一致。
此外，处理器有“存储缓冲区”来卸载对这些缓存的写入，以及“无效队列”，以便缓存一致性协议可以在写入即将发生时快速确认无效消息以提高效率。

这对数据意味着任何值的最新版本都可以在写入后的任何阶段位于寄存器、存储缓冲区、多级缓存之一或主存中。
如果线程要共享此值，则需要以有序的方式使其可见，这是通**过缓存一致性消息**的协调交换来实现的。
这些消息的及时生成可以通过内存屏障来控制。

读取内存屏障在 CPU 上命令加载指令， CPU 通过在无效队列中标记一个点来为进入其缓存的更改执行它。
这为它提供了一个一致的领域视图，用于在读取屏障之前有序的写入操作。

写屏蔽命令将指令存储在 CPU 上， CPU 通过在存储缓冲区中标记一个点来执行它，从而通过其缓存刷新写出。
这个屏障为领域提供了一个有序的视图，即在写屏障之前发生了什么存储操作。

完整的内存屏障对加载和存储进行排序，但仅在执行它的 CPU 上进行排序。

出了这三个原语之外，一些 CPU 还有更多变体，但这三个足以理解所涉及的复杂性。
在 Java 内存模型中， `volatile` 字段的读写分别实现了读写屏障。
这在 Java 5 发布时定义的 Java 内存模型^[3]中明确说明。

## 2.4. 缓存行 Cache Lines

在现代处理器中使用缓存的方式对于成功的高性能操作非常重要。
此类处理器在高速缓存中处理数据和指令时非常高效，但相对而言，当发生高速缓存未命中时效率非常低。

我们的硬件不会以字节或字为单位移动内存，为了提高效率，缓存被组织成大小通常为 32-256 字节的缓存行，最常见的缓存行是 64 字节。
这是缓存一致性协议运行的粒度级别。
这意味着如果两个变量在同一个缓存行中，并且它们由不同的线程写入，那么它们会出现与单个变量相同的写争用问题。
这是一个被称为“**虚假共享**”("false sharing")的概念。
因此，为了获得高性能，如果要最大限度地减少争用，则必须确保独立但并发写入的变量不共享相同的缓存行。

当以可预测的方式访问内存时， CPU 能够通过预测下一个可能访问的内存并将其后台预取到缓存中来隐藏访问主内存的延迟成本。
这只有在处理器可以检测到访问模式时才有效，例如具有可预测“**步幅**”的行走内存，当遍历数组的内容时，步幅可预测的，因此内存将被预取到缓存行中，从而最大限度地提高访问效率。
步幅通常必须在任意方向上都小于 2048 字节才能被处理器注意到。
然而，像链表和树这样的数据结构往往有更广泛地分布在内存中的节点，没有可预测的访问步幅。
内存中缺乏一致的模式限制了预取缓存行的能力，导致主内存访问效率降低 2 个数量级以上。

## 2.5. 队列的问题 The Problems of Queues

队列通常使用链表或数组作为元素的底层存储。
如果允许内存中队列不受限制，那么对于许多类问题，它可以不受控制地增长，直到它通过耗尽内存达到灾难性故障点。
当生产者超过消费者时，就会发生这种情况。
无界队列在保证生产者不会超过消费者并且内存是宝贵的资源的系统中很有用，但如果这个假设不成立并且队列无限制地增长，那么总会有风险。
为了避免这种灾难性的结果，队列的大小通常受到限制（有界）。
保持队列有界要求它是数组支持的或者主动跟踪大小。

队列实现往往会在头、尾和大小变量上发生写入争用。
在使用时，由于消费者和生产者之间的速度差异，队列通常总是接近满或接近空。
它们很少在生产率与消费率势均力敌的平衡中间地带运行。
这种总是满或总是空的倾向会导致高水平的争用和/或昂贵的缓存一致性。
问题在于，即时头尾机制使用不同的并发对象（如锁或 CAS 变量）分开，它们通常也会占用相同的缓存行。

管理生产者声明队列头部、消费者声明队列尾部以及中间节点存储的问题使得并发实现的设计非常复杂，难以管理，而不仅仅是在队列上使用单个大粒度锁。
用于放置和获取操作的整个队列上的大粒度实现起来很简单，但代表了吞吐量的重大瓶颈。
如果并发关注点在队列的语义中被分开，那么除了单个生产者-单个消费者实现外，实现变得非常复杂。

在 Java 中，队列的使用还有一个问题，因为它们是垃圾的重要来源。
首先，必须分配对象并将其放入队列中。
其次，如果支持链表，则必须分配代表链表节点的对象。
当不再被引用时，所有这些为支持队列实现而分配的对象都需要被回收。

## 2.6. 管道与图 Pipelines and Graphs

对于许多类别的问题，将几个处理阶段连接到管道中是有意义的。
此类管道通常具有并行路径，被组织成类似图的拓扑。
每个阶段之间的链接通常由队列实现，每个阶段都有自己的线程。

这种方法并不廉价——在每个阶段，我们都必须承担排队和取消排队工作单元的成本。
当路径必须分叉时，目标的数量会成倍增加此成本，并且在路径分叉后必须重新加入时，会产生不可避免的争用成本。

如果可以表达依赖关系图而不会产生在阶段之间放置的成本，那将是理想的。

# 3. LMAX Disruptor 的设计 Design of the LMAX Disruptor

## 3.1. Memory Allocation

## 3.2. Teasing Apart the Concerns

## 3.3. Sequencing

## 3.4. Batching Effect
